{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e367cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99378e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e2067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(73512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcaa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(r'test.pkl', 'rb') as f:\n",
    "    verify_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bab805-c345-4e51-a1a8-37310478117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ -1.,  -1.,  -1., ...,  78.,  40., 144.]), 0)\n",
      "(array([ -1.,  -1., 144., ...,  32.,  -1.,  -1.]), 0)\n",
      "(array([ 66., 100., 148., 148., 146.,  64., 146., 148.,  82.,   0.,  82.,\n",
      "       100.,  34., 132., 180.,  65.,  80.,  81., 131.,  52.,  34.,  52.,\n",
      "        64.,  52.,   3.,  66., 147.,  20.,   4., 132., 132., 100., 111.,\n",
      "        74., 110.,  60.,  92.,  65., 100., 189.,  44.,   8.,   5.,  76.,\n",
      "        31., 159.,   5., 124.,   4.,  12.,  51., 157.,  57.,  31., 183.,\n",
      "        57.,  65.,  92.,  69., 124., 122.,  79., 110.,  76.,  12.,  12.,\n",
      "        12.,  12.,   8., 159.,  12., 156., 100., 111.,  52., 121.,  36.,\n",
      "        47.,  41.,  41.,   8., 172.,  38.,  12.,  78.,  12.,  88.,  47.,\n",
      "       119.,  20.,  47.,  12., 159.,  20.,  20.,  76.,  60., 110., 132.,\n",
      "       185., 120.,  45., 110., 110.,  72., 124., 178.,  73.,   8.,  78.,\n",
      "        12.,  42., 173.,  12., 150.,  84.,  66., 152.,  69.,   8.,  41.,\n",
      "       159.,   5.,  78.,  44., 180.,  71.,  92., 152.,   6.,  12.,  45.,\n",
      "        92., 125.,   8., 156.,  12.,  12., 146.,   8., 185.,   0.,  88.,\n",
      "        92.,  90.,  12.,  12.,  12.,  78.,  92.,   3.,  45., 111., 111.,\n",
      "        71., 137.,  69.,  12.,  88.,  12.,  10.,   8., 172., 156., 152.,\n",
      "       100.,  12.,  12.,  77.,  52., 156., 148.,  12.,  12., 132.,   6.,\n",
      "         6., 185.,  47., 121.,  20.,  90.,  64., 111.,  92., 111.,  77.,\n",
      "        76.,  69., 100.,   8.,  76.,  76.,  69., 132.,   6.,   6.,   8.,\n",
      "        20.,  73.,  77., 111.,   6.,   8.,  13.,  12., 178.,   5.,  88.,\n",
      "       159., 121.,  58.,  44.,  47., 100., 156.,  12.,  41.,  88.,  73.,\n",
      "        20., 125.,  12., 124.,   5.,   0.,   5.,  47.,  12.,  12.,  12.,\n",
      "        12.,   8., 159., 185., 124., 100., 111.,  52., 121.,  36.,  47.,\n",
      "        41.,  41., 180.,   8.,  66.,   8., 146., 180.,  88.,  47., 119.,\n",
      "        20.,  73., 172., 158.,  20.,  20.,  20.,  78.,  69.,  12.,  12.,\n",
      "        47.,  47.,  47.,  30.,   8.,   6.,  45.,  12., 111., 111.,  38.,\n",
      "        12.,  12.,  12., 148.,   6.,   6., 185.,  47., 121.,  20.,  78.,\n",
      "        64., 111.,  77., 111.,  77.,  76., 141., 100.,   8.,  76.,  76.,\n",
      "         6., 132., 100., 111.,  74., 110.,  60., 159.,  79.,   5., 189.,\n",
      "       110.,  12.,   5.,  76.,  10.,  92.,   5., 124.,   4.,  12.,  51.,\n",
      "       157.,  57.,  31., 183.,  57.,  65.,  92.,  69., 124., 122.,  79.,\n",
      "       110.,  76.,  10.,  12.,  12.,  12.,   8., 159.,  12., 156., 100.,\n",
      "       111.,  52., 121.,  36.,  47.,  41.,  41.,   8., 172.,  38.,  12.,\n",
      "        78.,  12.,  88.,  47., 119.,  20.,  47.,  12.,  12.,  78.,  63.,\n",
      "        76.,  60., 110., 159., 132., 124.,  12.,  15., 110., 110.,  72.,\n",
      "       124., 178.,  73.,   8.,  78.,  12.,  12.,  30.,  12., 150.,  84.,\n",
      "        66., 152.,  69.,   8.,  41., 159.,   5.,  78.,  44., 180.,  71.,\n",
      "        92., 156., 132.,  79.,  45.,  92., 125.,   8., 156.,  12.,  12.,\n",
      "       146.,  12., 185.,   0.,  92., 108.,  90.,  47.,  12.,  12., 125.,\n",
      "        92.,   3.,  45., 111., 111.,  71., 137.,  69.,  12.,  47.,  12.,\n",
      "        10.,   8.,  41., 156., 152., 100.,  12.,  12.,  74.,  52., 156.,\n",
      "       148., 148.,  12.,  69.,   6.,   6., 185.,  47., 121.,  20.,  90.,\n",
      "        64., 111.,  92., 111.,  77.,  76., 141., 100.,   8.,  76.,  76.,\n",
      "        69., 132., 100.,   6.,   8.,  88.,  47., 141.,  77.,   6.,   8.,\n",
      "       152.,  12., 178.,   5.,  88., 159., 121.,  58.,  44.,  47., 100.,\n",
      "       156.,  12.,  41.,  88.,  73.,  20., 125.,  12., 124.,   5.,   0.,\n",
      "         5.,  47.,  12.,  12.,  12.,  12.,  12., 159., 185., 124.,   6.,\n",
      "       111.,  52., 121.,  36.,  47.,  41.,  41., 180.,   8.,  66.,   8.,\n",
      "       146., 180.,  88.,  47., 119.,  20.,  73.,  12., 158.,  20.,  20.,\n",
      "        20.,  78.,  69.,  12.,  12.,  47.,  47.,  47., 125.,   8.,   6.,\n",
      "        12.,  12., 111., 111.,  38.,  12.,  12.,  12.,  82.,   6.,   6.,\n",
      "       185.,  47., 121.,  88.,  90.,  64., 111.,  92., 111.,  77.,  76.,\n",
      "       141., 100.,   8.,  76.,  76.,  69., 132.,   6.,   6.,   8.,  47.,\n",
      "        47.,  10.,  10.,  82.,   8., 148.,  38.,  50.,  50.,  34.,  34.,\n",
      "        34.,  34.,  34.,  34., 144., 144., 144., 144.,  -1.]), 0)\n",
      "(array([147.,  65.,  64., 146.,   8.,  50., 160.,  50., 100.,  66., 144.,\n",
      "        66.,   3., 146.,  32., 148.,  12.,  17.,  13., 132.,  69.,  37.,\n",
      "         5., 180., 188., 127.,  60.,  20.,  88., 180.,  41., 153., 168.,\n",
      "        47.,  12., 145.,  12.,  69., 156.,  37.,  12.,  92.,  12., 132.,\n",
      "       100., 100.,  12., 156., 156., 132., 147.,  47.,  12.,  69.,  12.,\n",
      "        37., 156.,   3., 140.,  12.,  28.,  60.,  78.,  78., 190.,  76.,\n",
      "       106., 159., 159.,  65.,  76.,  74.,  60.,   0.,  60., 124., 140.,\n",
      "        12., 190., 190.,  76.,   0., 121.,  73.,  12.,  92.,  92.,   5.,\n",
      "         6., 132.,   8.,  69.,  12.,  38.,  90.,  38.,   8.,  45., 124.,\n",
      "         5.,   5., 120.,  78., 180.,  44., 180.,  12.,  77.,  92.,  88.,\n",
      "        12.,  78.,  92., 156.,  76.,  20.,  72., 156.,  78.,  78., 172.,\n",
      "        12.,  52., 141., 148.,  47.,  47.,  78.,  30.,  76., 106.,  14.,\n",
      "        76.,  77.,  44.,  45.,  12.,  13.,  47.,  44.,  42., 156.,   8.,\n",
      "       156.,  30.,  44., 121.,  78.,  20.,  78.,  22., 127., 125., 122.,\n",
      "        33.,  60.,  78., 153., 116., 116.,  95., 156., 156.,  76.,  76.,\n",
      "       111.,  40.,  79.,  88.,  45., 186., 156., 124., 117.,  20.,  20.,\n",
      "        79., 156., 159.,  47.,  92.,  64., 156.,  12.,  30.,  25., 156.,\n",
      "       111.,  44.,  44., 111.,  77.,  12.,  90.,  74.,  34., 142., 111.,\n",
      "        41.,   3.,  14.,  73.,  41.,  30., 111.,  92.,  30.,  78., 172.,\n",
      "       159.,  50., 152., 156.,  50., 160.,  50., 100.,  66., 144.,  66.,\n",
      "         3., 146.,  32., 148.,  12.,  17.,  13., 132.,  69., 100.,   5.,\n",
      "       180., 188., 127.,  60.,  20.,  88.,  73.,  41.,  68., 168.,  47.,\n",
      "        12., 145.,  12.,  69., 156.,  37.,  12.,  92.,  12., 132., 100.,\n",
      "       100.,  12., 156., 156., 132., 147.,  47.,  47.,  69.,  12.,  37.,\n",
      "       156.,   3., 140.,  12.,  12.,  60.,  78.,  78., 190.,  60., 106.,\n",
      "       159., 159.,  65.,  76.,  74.,  60.,  52.,  60., 124., 140.,  12.,\n",
      "       190., 190.,  76.,   0., 121.,  73.,  47.,  92.,  92.,   5.,   6.,\n",
      "        37., 156.,  69.,  12., 172.,  90.,  38.,   8.,  45., 124.,   5.,\n",
      "         5., 120.,  78., 117.,  44.,  44.,  73.,  30.,  92.,  88.,  12.,\n",
      "        92.,  92., 156.,  76.,  20.,  72.,  92.,  78.,  78., 172.,  45.,\n",
      "        52., 141., 148.,  47.,  47.,  78.,  30.,  76.,  92.,  14.,  76.,\n",
      "        77., 111.,  14.,  12.,  13.,  47.,  44.,  42., 156.,   8.,  37.,\n",
      "        30.,  44.,  76.,  78.,  20.,  78.,  22., 127., 125., 122.,  33.,\n",
      "        60.,  78.,  92., 188., 191.,  95., 156., 156.,  76.,  72., 111.,\n",
      "        40.,  79.,  88.,  45., 186., 156., 124.,  44.,  20.,  20.,  79.,\n",
      "       156., 159.,  47.,  92.,  64., 156.,  12.,  44.,  25.,  44., 111.,\n",
      "        44.,  44., 111.,  77.,  12.,  90.,  74.,  34., 142., 111.,  41.,\n",
      "       100., 110.,  44., 148.,  12., 111.,  76.,  30.,  78., 172., 159.,\n",
      "        50., 152., 156.,   5.,  76.,  44., 156.,   8.,   6., 156.,  31.,\n",
      "        13.,  20., 146., 146.,  67.,   3., 178., 100.,  34., 144., 144.,\n",
      "       144., 144., 144., 144., 144., 144.,  -1.]), 0)\n",
      "(array([144., 144., 144., 144., 144.,  64.,  64.,  64.,   0., 112., 112.,\n",
      "       112., 112., 176., 176., 176.,  82.,   0.,   0.,   0.,  82., 176.,\n",
      "        66., 144.,  34., 128., 128., 128., 128., 131.,  65.,  18.,  50.,\n",
      "       144.,  68.,   0.,  68.,  34.,  68.,  52.,   0.,   4.,   4.,   0.,\n",
      "       106.,   0.,  68., 131.,  36.,  81.,  84.,  66.,  88.,  67.,  12.,\n",
      "        35., 148.,   3.,   0.,   2., 100.,   6.,   6.,   8.,  12.,  69.,\n",
      "        38.,  34., 152., 146.,  40.,  24., 159., 110., 122., 112.,  26.,\n",
      "       112., 189.,  90.,  61.,   4.,  76.,  92., 124., 112., 189., 183.,\n",
      "       111., 137.,  12.,   1.,  42.,   3.,  77., 138., 124., 144.,  25.,\n",
      "        30.,  44.,  33.,  47.,   3.,  12., 177., 124.,  34.,  37.,  64.,\n",
      "         6., 148.,  12., 152., 111.,  73., 157.,  37., 124., 150.,  88.,\n",
      "        72.,  62., 106., 106.,   0.,  74.,  42.,  78.,  43., 156., 172.,\n",
      "        47., 106., 141.,  71.,  12.,  69.,  77.,  77.,  92.,   8.,  12.,\n",
      "        47.,  78.,  64.,  69.,  67.,  88.,  33.,  60., 142., 156.,  40.,\n",
      "        88., 147.,  56.,  34.,  37.,  30.,  44., 120.,  37.,  35.,  56.,\n",
      "         1.,  28.,  14., 124.,   8., 117., 115.,  24.,  18.,   5.,  94.,\n",
      "        12.,   0.,   0.,   0.,  70., 117.,  79.,  32.,  47.,  31.,  31.,\n",
      "       106.,   5., 119.,  47.,  17.,  42.,  12., 125.,  66.,   8.,  41.,\n",
      "        78.,  71.,  12.,  69., 159., 111.,  92.,   8.,  78.,  40.,  40.,\n",
      "        44.,  44.,  78.,  44.,  41.,  30.,   5.,   5., 156.,  47.,  77.,\n",
      "        92.,   5.,  13.,   4.,   4.,   4.,  54.,  50.,  76.,  73., 117.,\n",
      "        14., 159., 110.,   7.,  26.,  12., 159.,  53.,  50.,  90.,  52.,\n",
      "        45.,   0., 106.,   5., 159., 183.,  76.,  65.,  37., 157.,  12.,\n",
      "       106.,  77., 152., 152.,  90.,  65.,  77., 156.,  25., 189.,   5.,\n",
      "         5.,  45.,  41.,  45., 124.,  37.,  13.,  68.,  68.,  60.,  13.,\n",
      "         7., 158., 121.,  92., 125., 185.,   8.,  12.,  12.,  12.,  47.,\n",
      "        47.,   5.,  13.,  20.,  88.,  47.,  12.,   8., 159., 119.,  47.,\n",
      "        73.,  12., 178.,  69.,  12.,  12., 106.,  92., 159., 159., 112.,\n",
      "        23.,  18., 108.,  94.,  12.,   0.,   0.,  67.,  88.,  33.,  74.,\n",
      "       142., 156.,  30.,  25., 144.,  22.,  34., 140.,  30.,  44., 120.,\n",
      "       120.,  32., 120., 120., 126., 173.,  44.,  57.,  12., 132.,  12.,\n",
      "        41.,  40.,  24., 159., 110., 110.,  26.,  79.,  40.,  12., 137.,\n",
      "       190., 190.,  12., 159.,   5.,  65.,  92.,  76.,  76.,  65.,  79.,\n",
      "        72.,  72.,  20.,  73.,  23.,   6.,   6., 124.,  12.,  44.,  20.,\n",
      "        47.,  88., 159., 124.,  12.,  92.,   5., 100., 185.,  93.,  12.,\n",
      "       180.,  47.,  56., 158.,  92.,  47.,  60., 121.,  68., 153.,  93.,\n",
      "       172.,  45.,  13.,  37.,  37., 158., 191.,  12., 127.,  44.,  44.,\n",
      "       121., 127.,  78.,  90.,  38., 172.,  80.,  92.,  92., 110.,  27.,\n",
      "        13.,  60., 127.,  38., 125., 151.,  92.,  19.,  30.,  44.,  44.,\n",
      "        78., 111., 124., 156.,  40., 156.,  30.,  44.,  44.,  44.,  45.,\n",
      "        47.,  72.,  72.,  45.,  13.,  68.,   5., 180., 180., 180., 172.,\n",
      "         8.,   8.,  88.,  92.,  71.,  12., 145., 106., 152.,   5., 159.,\n",
      "       109.,  44.,  76., 110., 110., 109.,  44.,  77.,  77.,  33.,  47.,\n",
      "       111.,  92.,  12.,  12.,  41.,  12., 183.,  76.,  78.,  30., 117.,\n",
      "        44., 151.,  44., 111.,  90.,  58.,  30.,  88.,  88.,  45., 156.,\n",
      "         6.,  12., 156.,  45.,  13.,  44., 121.,  79.,  60.,  47.,  72.,\n",
      "        62., 106., 106.,   0.,  77.,  12.,  30.,  43.,  30., 156.,  92.,\n",
      "         5.,  78.,  38.,  12.,  12.,  73., 141.,  92.,  47.,  47.,  40.,\n",
      "        44.,  44.,  20.,  36., 121.,  92., 125.,  44.,  44.,  88.,  88.,\n",
      "        33.,  88., 172., 172., 118., 121.,  58.,  44., 156.,  38.,  38.,\n",
      "        12.,  92., 156.,  40., 146.,  68., 121.,  33.,  88.,  40.,  40.,\n",
      "        88.,  88.,  91.,  91.,  88.,  88.,  33.,  33.,  33.,  44.,  44.,\n",
      "        33., 132.,  30.,  30.,  30., 144., 144., 144., 144., 144., 144.,\n",
      "       144., 144., 144., 144., 144., 144., 144., 144., 144.]), 0)\n",
      "(array([128.,  50.,  67.,   3.,  12.,  78.,  44.,  40.,  44.,  60.,  78.,\n",
      "       156.,  92., 141.,  77., 156.,  76., 159., 159.,  12.,  44.,  47.,\n",
      "        78., 125., 125.,  12., 159.,  94.,  78.,  47.,  30.,  12.,  28.,\n",
      "        28.,  12.,  13., 125.,  47.,  93.,  28.,  92.,  28.,  76.,  30.,\n",
      "        12.,  12., 156.,  45.,  77.,  78.,  12., 157., 159.,   5.,  12.,\n",
      "       156.,  47.,  77.,  12.,   8.,  12.,  12.,  12.,  12.,  78.,  77.,\n",
      "        77., 141., 157., 159.,  92.,  47.,  12.,  77., 156.,  15.,  79.,\n",
      "       125.,  47.,  28., 159., 141.,  92.,  92.,  47., 156.,  78., 125.,\n",
      "        47., 156.,  30.,  28.,  28.,  78.,  78.,  92.,  28., 125.,  78.,\n",
      "        78.,  47., 156.,  12.,  15.,  92.,  79.,  15., 190.,  76.,  92.,\n",
      "        62., 110., 157., 189.,  77.,  12.,  78.,  78.,  12.,  45., 124.,\n",
      "        44.,  44., 125.,  12.,  93.,  93.,  88., 159., 190.,  12.,  92.,\n",
      "        30.,  28.,  78.,  78.,  12.,  28.,  28.,  12.,  28., 157., 125.,\n",
      "       157.,  15.,  44.,  76., 125.,  12.,  28.,  47.,  28.,  28.,  28.,\n",
      "        12.,  28.,  28.,  77.,  12.,  28.,  12.,  92.,  94.,  77.,  92.,\n",
      "       159.,  44., 125.,  77.,  78.,  12.,  12.,  78.,  28.,  78.,  12.,\n",
      "        12.,  12.,  44.,  44.,  47.,  12.,  12.,  12.,  92.,  28.,  28.,\n",
      "        92.,  13.,  28.,  28.,  28., 158.,  78.,  30.,  13.,  28., 127.,\n",
      "        28.,  92.,  28.,  28.,  12., 127.,  92., 158., 125.,  13.,  28.,\n",
      "        28.,  28., 125.,  12.,  28.,  28.,  92.,  92., 127., 127.,  44.,\n",
      "       172.,  78.,  78.,  92.,  28.,  47.,  78.,  12.,  12.,  78.,  47.,\n",
      "        13.,  28.,  28.,  78.,  12.,  12.,  12.,  44., 144.]), 0)\n",
      "(array([ 34.,  34.,  68.,   0.,   0.,   0.,   0.,   0., 100.,  34.,  34.,\n",
      "        34.,  84.,  18.,  84.,  34., 100., 112., 112.,   0.,   0., 114.,\n",
      "       114., 112.,   5.,  68.,  68., 100., 185.,   6., 118.,   0.,   0.,\n",
      "        52.,  52.,  68.,  76.,  90.,  74.,  52.,  52.,  76.,  76.,  73.,\n",
      "       112., 100., 100.,   5.,  79., 159.,  68.,  92.,  92.,  79., 140.,\n",
      "       159.,   5.,   5.,  79.,  79.,  92.,  92.,  92.,   5., 125.,  13.,\n",
      "        60.,  74.,  13., 159.,   5., 111.,  12.,   6.,  13.,  92.,  92.,\n",
      "        12.,  12.,  12.,  12., 185., 185.,  93.,  28.,  12.,  12.,  76.,\n",
      "        76.,  92., 190., 124.,  76.,  65.,  65.,  76.,  76., 110.,   5.,\n",
      "        92.,  92., 125.,  28.,  47.,   0.,  47.,  47.,  12.,  28.,  12.,\n",
      "        12.,  12.,  12., 125.,  78.,  12.,  12.,  12.,  12., 159., 156.,\n",
      "        12.,  45.,  15.,  79.,  78.,  12.,  12., 190., 159.,  12., 159.,\n",
      "        12.,  78.,  12.,  92., 125., 125.,  78.,  92.,  92.,  13.,  13.,\n",
      "        13.,  92.,  93., 125., 121., 121., 157., 127., 127.,  44.,  44.,\n",
      "       127., 158., 127.,  60.,  60.,  20.,  60., 125.,  37.,  34., 127.,\n",
      "        60.,  74., 127.,  78.,  13., 127., 157.,  44., 121.,  60.,  13.,\n",
      "        47.,  28.,  15., 124., 158., 100., 153., 127.,  13.,  92.,  28.,\n",
      "       127., 158.,   6., 127.,  28.,  92.,  13., 127., 158., 127.,  13.,\n",
      "        47.,  13.,  28., 158., 127., 127., 158., 125., 127., 110., 110.,\n",
      "       109.,  78.,  60.,  28.,  92., 158.,  29.,  15.,  15.,  93.,  29.,\n",
      "        15., 127., 158., 153., 153.,  60.,  92., 127., 127., 157.,  47.,\n",
      "        13.,  92.,  34.,  44., 121.,  60.,  44.,  30.,  93.,  12.,  92.,\n",
      "        60.,  92.,  92.,  77.,  12.,  12.,  13., 159.,  78.,  78.,  12.,\n",
      "        12., 159.,  12.,  12., 159., 111.,  78.,  78.,  12.,  92., 125.,\n",
      "        78.,  12.,  12.,  28.,  45.,  12., 190.,  12.,  12., 156., 190.,\n",
      "        12.,  13.,  44.,  30.,  44.,  60.,  13.,  12.,  78.,  78., 124.,\n",
      "        12.,  15., 159., 159.,  12.,  78.,  92.,  78.,  92.,  12.,  13.,\n",
      "       159., 111.,  77.,  12., 159., 159.,  12.,  12., 159.,  79.,  60.,\n",
      "        12.,  12.,  12., 111., 159.,  12.,  12.,  12.,  12., 159., 156.,\n",
      "        44., 125., 125., 157.,  44.,  60., 157.,  44.,  44., 125., 125.,\n",
      "       125., 157.,  44.,  44., 151., 151., 151., 112., 112.,  -1.,  -1.,\n",
      "        -1.]), 0)\n",
      "(array([144., 132.,  12.,  20.,  20., 180.,  12.,  69.,  12.,  12.,  78.,\n",
      "        12., 124.,   8.,  12.,  47.,  77., 159., 159.,  12.,  12., 157.,\n",
      "       190.,  65.,  92., 119.,  78.,  69., 125.,  88.,  79.,  37.,  28.,\n",
      "        12.,  47.,  12.,  30.,  78.,  12.,  12.,  78.,  30.,  94., 141.,\n",
      "        12.,  41.,  47.,  30.,  14.,   9.,  13., 159.,  28.,  60.,  15.,\n",
      "       158.,  92.,   5.,  13.,  15.,  29., 158., 127., 125.,  28.,  92.,\n",
      "        92.,  47.,  28., 158.,  92.,   7.,  28., 127.,  28.,  14.,  45.,\n",
      "        60., 127.,  44.,  44., 121.,  28., 112.,  28.,   0.,  28.,   8.,\n",
      "        28.,  30.,  92.,  78.,  77.,  78.,  78.,  12.,  77.,  78.,  94.,\n",
      "        47.,  12.,  41.,  47.,  33.,  45.,  73.,  12.,  65.,  92.,   0.,\n",
      "        13.,  80.,  47., 121.,  78.,  43.,  30.,  43.,  78.,  45., 141.,\n",
      "        12.,  47.,  77.,  77., 125., 157., 141.,  12.,  69., 141.,  12.,\n",
      "        12., 111., 141.,  47.,  78.,  47.,  28.,  44.,  44., 144.]), 0)\n",
      "(array([147.,  65.,  64., 146.,   8.,  50., 160.,  50., 100.,  66., 144.,\n",
      "        66.,   3., 146.,  32., 148.,  12.,  17.,   6., 132.,  69., 100.,\n",
      "         5., 180., 188.,  44.,  20.,  20.,  88., 180.,  41.,  68., 168.,\n",
      "        47.,  12., 145.,  12.,  69., 156.,  37.,  12.,  92.,  12., 132.,\n",
      "       100., 100.,  12., 156., 156.,  66., 147.,  47.,  12.,  69.,  12.,\n",
      "        37.,   8.,   3., 140.,  12.,  28.,  60.,  78.,  71., 190.,  60.,\n",
      "       106., 159.,  92.,  65., 124.,  74., 110.,   0.,  60., 124., 140.,\n",
      "        12., 190., 183.,  76.,   0., 121.,  73.,  47.,  92.,  92.,   5.,\n",
      "         6., 132.,   8., 145.,  12.,  38.,  90.,  38.,   8., 159., 124.,\n",
      "        68.,   5., 120.,  78., 180.,  44.,  44.,  73.,  23.,  92.,  88.,\n",
      "        12.,  78.,  92., 156.,  76.,  20.,  72.,  92.,  78.,  78.,   8.,\n",
      "        12.,  52., 141., 148.,  47.,  47.,  78.,  30.,  60., 106.,  69.,\n",
      "        76.,  58.,  44.,  70.,  12.,  71.,  47.,  44.,  44.,  45.,   8.,\n",
      "        37.,  30.,  60., 121.,  78.,  20.,  20.,  22.,  44., 125., 122.,\n",
      "        33.,  60.,  78., 153., 116., 140.,  95., 156., 156.,  76.,  72.,\n",
      "       111.,  40.,  79.,  88.,  45., 186., 156.,  14.,  73.,  20.,  20.,\n",
      "        79., 156., 159.,  47.,  92.,  64., 156.,  12.,  44.,  25.,  44.,\n",
      "       111.,  44.,  44., 111.,  77.,  12.,  90.,  74.,  34., 142.,  52.,\n",
      "        41., 100., 110.,  73.,  41.,  30., 111.,  92.,  30.,  73., 172.,\n",
      "       159.,  50.,  17., 156.,  50.,  50.,  50., 100.,  66., 144.,  66.,\n",
      "         3., 146.,  32., 148.,  12.,  17.,   6., 132.,  69., 100.,   5.,\n",
      "       180., 188.,  44.,  20.,  20.,  88., 180.,  41.,  68., 168.,  47.,\n",
      "        12., 145.,  12.,  69., 156.,  37.,  12.,  92.,  12., 132., 100.,\n",
      "       100.,  12., 156., 156.,  66., 147.,  47.,  12.,  69.,  12.,  37.,\n",
      "         8.,   3., 140.,  12.,  28.,  60.,  78.,  71., 190.,  60., 106.,\n",
      "       159.,  92.,  65., 124.,  74., 110.,   0.,  60., 124., 140.,  12.,\n",
      "       190., 183.,  76.,   0., 121.,  73.,  47.,  92.,  92.,   5.,   6.,\n",
      "       132.,   8., 145.,  12.,  38.,  90.,  38.,   8., 159., 124.,  68.,\n",
      "         5., 120.,  78., 180.,  44.,  44.,  73.,  23.,  92.,  88.,  12.,\n",
      "        78.,  92., 156.,  76.,  20.,  72.,  92.,  78.,  78.,   8.,  12.,\n",
      "        52., 141., 148.,  47.,  47.,  78.,  30.,  60., 106.,  69.,  76.,\n",
      "        58.,  44.,  70.,  12.,  71.,  47.,  44.,  44.,  45.,   8.,  37.,\n",
      "        30.,  60., 121.,  78.,  20.,  20.,  22.,  44., 125., 122.,  33.,\n",
      "        60.,  78., 153., 116., 140.,  28., 156., 190., 110.,  72., 111.,\n",
      "        40.,  79.,  88.,  45., 186., 156.,  14.,  73.,  20.,  20.,  79.,\n",
      "       156., 159.,  47.,  92.,  64., 156.,  12.,  44.,  25.,  44., 111.,\n",
      "        44.,  44., 111.,  77.,  12.,  90.,  74.,  34., 142.,  52.,  41.,\n",
      "       100., 110.,  73.,  41.,  30., 111.,  92.,  30.,  73., 172.,  73.,\n",
      "        64.,  17., 156.,   5., 110.,  44., 156.,   8., 132., 156., 159.,\n",
      "        20., 146.,  12., 178., 144., 144., 144.]), 0)\n",
      "(array([ -1.,  -1., 144., 144., 144.,  64.,  64., 146.,   1.,   0.,   0.,\n",
      "         0.,   1.,  32.,  32.,  32., 146.,  64.,  64.,  64., 144., 144.,\n",
      "       145.,  67., 145., 100., 100.,  66.,   5.,  65.,  65.,  20.,   6.,\n",
      "         3., 147.,  33.,  38.,  35.,  64., 100.,   3., 145.,  68.,  68.,\n",
      "        33., 146., 152.,  17.,  13., 145.,  88.,  80., 172., 180., 112.,\n",
      "        37.,  12.,   0.,  24., 151.,  44., 121.,  88.,  30.,  25., 110.,\n",
      "       189., 183.,  76.,  65.,   0.,   0.,  92.,  80.,  33.,  32., 124.,\n",
      "       112.,   5.,  71., 156.,  12.,  69., 106., 159.,   5.,  88.,  92.,\n",
      "       124.,   9.,  55., 116., 125.,  41.,  46., 190.,  60., 110.,  92.,\n",
      "        73.,  12.,  92., 106.,  37., 124.,  77., 127.,  74.,  13., 158.,\n",
      "        22.,  44.,  79., 191.,  44.,  60.,  77., 125.,  12.,  26.,  44.,\n",
      "        60.,  88.,  38.,  47.,  45.,  92.,   8., 172.,  38.,  78.,  30.,\n",
      "       156., 151., 118., 124., 141., 156.,  12.,  41.,  12., 178., 152.,\n",
      "        71., 156.,  12.,  12.,   5.,  92.,  47.,  47.,  33.,  60.,  20.,\n",
      "         0.,  12.,  28.,  88.,  88.,  92.,  28.,  12.,  41.,  77.,  12.,\n",
      "        92.,  14., 105.,  44.,  37.,   8., 120., 120., 159.,  37., 119.,\n",
      "        78.,  12.,  73.,   0.,  88.,  47.,  88.,  41., 125.,  77., 127.,\n",
      "        60.,  13.,  41., 172., 151.,  44.,  73.,  12., 172., 125.,  77.,\n",
      "       127.,  60.,  44.,  40., 172.,  34., 121.,  88.,  88.,  41.,  94.,\n",
      "       190., 124., 119.,  20.,  65.,  88.,  88., 159.,  12.,   8., 185.,\n",
      "       124., 190.,  30., 174., 158.,  44.,  44.,  60., 125.,  73.,   0.,\n",
      "        92.,  47., 172.,  56.,  60., 158., 151.,  40.,  45.,  78.,  12.,\n",
      "        69., 148.,  69., 111., 137.,  12.,  12.,  41.,  41., 158.,  28.,\n",
      "        40.,  85.,  56.,  60., 127.,  13.,   5.,  92.,  45.,  88.,  41.,\n",
      "        93., 119.,  47.,   0.,  47.,  93., 127.,  60.,  28.,   7.,   7.,\n",
      "        66., 158., 114.,   0.,   7.,  92.,   5.,  89.,   6., 172.,  15.,\n",
      "        85.,  60.,  92., 158.,   5., 172.,  28.,  92., 158.,  44., 127.,\n",
      "       160., 160.,  74.,  47.,  92.,   5., 158.,  93.,  12.,  20., 127.,\n",
      "       127., 151.,  60.,  47.,  28.,  74., 172., 151.,  44.,  60.,  88.,\n",
      "       125.,  78.,  69.,  69., 148.,  12., 111., 137.,  78.,  78.,  80.,\n",
      "        80., 152., 172., 160., 113., 127.,  60., 127., 172., 151.,  44.,\n",
      "        47.,  47., 153., 158.,  13.,   6.,   9.,  13.,  92., 153.,  92.,\n",
      "       172.,  13.,  92.,   7.,  13.,  47.,  47., 158., 158.,  92.,   5.,\n",
      "        88., 172.,  14., 158., 121.,  47.,  41.,  41., 158.,  92., 153.,\n",
      "        66., 153.,  92., 157.,  73.,   7.,  92.,   7., 172.,  41., 172.,\n",
      "        12.,  20., 117.,  44., 157.,  44.,  44.,  44.,  45., 190.,  46.,\n",
      "        94.,  88.,  12.,  71.,  78., 156., 152.,  80.,   9.,  71.,  78.,\n",
      "       119.,  12.,  45.,  13., 159.,  13.,  41.,  12.,  47.,  78.,  12.,\n",
      "        92.,  41.,  69., 178., 152.,  71.,  12.,  92.,  12.,  13.,  13.,\n",
      "        44.,  85.,  56.,  47., 111., 137.,  71., 156.,  12., 159.,  92.,\n",
      "        88.,  47.,  47.,  47.,  78.,  69.,  69.,  41.,  12.,  12.,  12.,\n",
      "        42., 152.,  41.,  12., 156.,  13.,  69.,  82., 132.,   3.,   3.,\n",
      "       148., 148., 146.,   1., 146.,  64., 180., 180.,  65.,  80.,  65.,\n",
      "       180., 132., 132., 148., 100.,  52.,  52.,  68.,  60.,  60.,  20.,\n",
      "        76.,  92.,   5.,  79., 191.,  79.,  52.,  60.,  69., 185.,   5.,\n",
      "        73., 156., 156., 185., 106.,  10.,  76.,  52.,  74.,  74.,  73.,\n",
      "       124., 124., 156.,  12.,  12., 156., 156.,  12.,  69.,  45.,  12.,\n",
      "        12.,  30.,  78.,  30.,  47.,  12., 159., 125.,  44.,  47.,  47.,\n",
      "        92.,  12.,  12., 125.,  78.,   5.,  79.,  45., 159.,   5.,  92.,\n",
      "        76.,  72.,  92.,  92.,  79., 159., 190., 124.,  76., 190., 159.,\n",
      "        68.,  79.,  13.,  12., 159.,  76.,  12.,  76.,  65.,  76.,  76.,\n",
      "        73., 156.,  37.,  13.,  41.,  12.,  45.,  44.,  44.,  88.,  13.,\n",
      "       124., 124.,  37.,  13.,  12.,  12.,  47.,  93.,  78.,  12.,  12.,\n",
      "       156.,  12.,  41.,  47.,   5.,  92.,  77.,   6., 132.,  69.,  92.,\n",
      "        12.,  12.,  12.,  41.,  13.,   0.,  47., 172.,  47.,  91.,  78.,\n",
      "        13.,  69.,  45.,  12.,  92.,  76., 189.,  92.,  92.,  44.,  44.,\n",
      "         5.,   5.,  92.,  45., 190.,  76.,  76.,  76.,  76., 159., 159.,\n",
      "        14.,  79.,  79., 124.,  76.,   5.,  92.,   5.,  79.,  12., 111.,\n",
      "       156.,  92.,  31.,  76.,  65.,  76.,  76.,  79., 124.,  45.,  13.,\n",
      "       125.,  12.,  13.,  13.,  12.,  92.,  13.,  12.,  92.,  12.,  12.,\n",
      "         0.,  47.,  12.,   3., 159.,  88.,  12.,  12.,   8.,  47., 180.,\n",
      "        41.,  12.,  13.,  28.,  47.,  92.,  12.,  20.,  12.,  12.,  12.,\n",
      "       159.,  69.,  28.,  28.,  47., 172.,  13., 158.,  92.,  68., 158.,\n",
      "        13.,  92.,  44.,  44., 125.,  44., 124.,  37., 127.,  60.,  74.,\n",
      "       116., 158.,  92.,  88., 172.,  47.,  12.,  12., 156.,  30.,  60.,\n",
      "       117., 110., 127.,  44., 127., 158., 125.,  44.,  47.,  60.,  13.,\n",
      "        14.,  14.,  14., 158., 140.,  46., 124., 120.,  37., 172.,  45.,\n",
      "        60.,  13.,  13.,  46., 124.,   6.,   5.,  93.,  78.,  12.,  85.,\n",
      "       125.,  44.,  12.,  12.,  92.,  44.,  44.,  36.,  37., 127., 127.,\n",
      "       127.,  61., 110., 110., 109., 172.,  13., 127.,  44., 118.,  36.,\n",
      "        33.,  80.,  65.,  64.,   0., 112.,   2.,  18., 100.,   3., 132.,\n",
      "       100., 138.,  36.,  40.,  17., 146.,  68.,  66.,  31.,  57.,  50.,\n",
      "        74.,  65.,  65.,  17.,   6., 145., 145.,  34.,  24.,  88.,  88.,\n",
      "        47.,  12.,  12.,  12., 109.,  79.,  77.,  12.,  41.,  47., 141.,\n",
      "        77.,  71., 156.,  58., 146., 153., 110.,  31.,  57., 183.,  76.,\n",
      "        65.,   5.,  17.,  47.,  88.,  88.,  47.,  31., 124.,  92., 127.,\n",
      "        40.,  92.,  88.,  38.,  78.,  71.,  12., 151.,  44., 124., 156.,\n",
      "       148.,  31., 111., 137., 156.,  12.,   5.,  13.,  44.,  44.,  47.,\n",
      "        78.,  78.,  12.,  12.,  78.,  47.,  31., 152.,  92.,  47.,  12.,\n",
      "       159.,  12.,   8., 124.,  44.,  47.,  93.,  31., 111.,  77.,  30.,\n",
      "        69.,  31.,  31.,  72.,  76.,  12., 159., 156.,  30.,  44.,  47.,\n",
      "        94.,  46.,  15., 158., 127.,  60.,  76., 189., 124., 120.,  12.,\n",
      "        78.,  43.,  78.,  44.,  44.,  74.,  92., 141.,  76., 141., 137.,\n",
      "        76.,  76.,  17.,  78.,  47.,  47.,  12.,  12.,  78.,  78.,  12.,\n",
      "        28.,  78.,  92.,  47.,  78.,   5., 145., 144.]), 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bbdd1",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f27cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categories = [row[1] for row in data]\n",
    "\n",
    "#giving weights to each class based on how many labels it has for more balanced training of neural network\n",
    "class_weights = torch.tensor(compute_class_weight('balanced', classes=np.array([0,1,2,3,4]), y=categories)).to(device) \n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "categories_2d = [[category] for category in categories]\n",
    "onehot_encoded = encoder.fit_transform(categories_2d)\n",
    "onehot_encoded_array = onehot_encoded.toarray()\n",
    "\n",
    "data_modified = []\n",
    "targets_modified = []\n",
    "for i in range(len(data)):\n",
    "    row = list(data[i][0])\n",
    "    data_modified.append(row)\n",
    "    row2 = onehot_encoded_array[i]\n",
    "    targets_modified.append(row2)\n",
    "data = [torch.tensor(row).to(torch.float32) for row in data_modified]\n",
    "targets = [torch.tensor(row).to(torch.float32) for row in targets_modified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588bc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VariableLenDataset(Dataset):\n",
    "    def __init__(self, in_data, target):\n",
    "        self.data = [(x, y) for x, y in zip(in_data, target)]      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data, target = self.data[idx]\n",
    "        return in_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92566e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_16084\\939005982.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  data = np.array(data,dtype=\"object\")\n",
      "C:\\Users\\-\\AppData\\Local\\Temp\\ipykernel_16084\\939005982.py:2: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  targets = np.array(targets,dtype=\"object\")\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data,dtype=\"object\")\n",
    "targets = np.array(targets,dtype=\"object\")\n",
    "train_indices = rng.random(len(data)) > 0.3\n",
    "test_indices = ~train_indices\n",
    "train_indices = np.where(train_indices==True)[0]\n",
    "test_indices = np.where(test_indices==True)[0]\n",
    "train_set = VariableLenDataset(data[train_indices], targets[train_indices])\n",
    "test_set = VariableLenDataset(data[test_indices], targets[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd768ab2-0b95-471c-8ae8-dc48032c6b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram to see classes distribution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "targets_flat = [np.argmax(target) for target in targets]\n",
    "targets_flat = [int(target.item()) for target in targets_flat]\n",
    "\n",
    "class_labels = np.arange(5)\n",
    "train_class_counts = {label: targets_flat.count(label) for label in class_labels}\n",
    "\n",
    "plt.bar(train_class_counts.keys(), train_class_counts.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10b9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "pad = 0\n",
    "\n",
    "#since sequences have different lengths, there's need for 'padding' to make them equal length\n",
    "def pad_collate(batch, pad_value=0):\n",
    "    xx, yy = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a4d26",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abc1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        out = all_outputs[-1]\n",
    "        x = self.fc(out)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a6455",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=10, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False, collate_fn=pad_collate)\n",
    "model = LSTMRegressor(1, 200, 4, 5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fun = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f952659",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.09\n",
      "Epoch: 10, loss: 0.581\n",
      "Epoch: 20, loss: 1.61\n",
      "Epoch: 30, loss: 1.35\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "for epoch in range(251):\n",
    "    for x, targets, x_len, target_len in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device) \n",
    "        \n",
    "        preds, _ = model(x, (hidden, state))        \n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "with torch.no_grad():\n",
    "    true_preds = 0\n",
    "    num_preds = 0\n",
    "    true_classes_preds = np.array([0,0,0,0,0])\n",
    "    num_classes_preds = np.array([0,0,0,0,0])\n",
    "    for x, targets, x_len, target_len in test_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device) \n",
    "        preds, _ = model(x, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        preds = (torch.round(preds).int()).long()\n",
    "        if (torch.argmax(preds) == torch.argmax(targets)):\n",
    "            true_preds +=1\n",
    "            true_classes_preds[torch.argmax( \n",
    "                targets).item()]+=1\n",
    "        num_classes_preds[torch.argmax(targets).item()]+=1\n",
    "        num_preds += targets.shape[0]\n",
    "acc = true_preds/num_preds\n",
    "classes_acc = true_classes_preds/num_classes_preds\n",
    "\n",
    "#Various indicators of model effectiveness or lack of it. Number of labels in classes is unbalanced, so accuracy can be misleading.\n",
    "print(\"Number of classes predictions:\",true_classes_preds)\n",
    "print(\"Number of classes labels\",num_classes_preds)\n",
    "print(\"Classes accuracy:\",classes_acc)\n",
    "print(\"Mean:\",statistics.mean(classes_acc))\n",
    "print(\"Euclidean norm:\",np.linalg.norm(np.array(classes_acc))/5)\n",
    "print(\"Accuracy:\",acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e198e",
   "metadata": {},
   "source": [
    "# Classify data with no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing - won't work if it has labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "verify_categories = [row[1] for row in verify_data]\n",
    "\n",
    "\n",
    "verify_data_modified = []\n",
    "for i in range(len(verify_data)):\n",
    "    row = list(verify_data[i])\n",
    "    verify_data_modified.append(row)\n",
    "verify_data = [torch.tensor(row).to(torch.float32) for row in verify_data_modified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VerifyVariableLenDataset(Dataset):\n",
    "    def __init__(self, in_data):\n",
    "        self.data = in_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data = self.data[idx]\n",
    "        return in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "pad = 0\n",
    "\n",
    "def verify_pad_collate(batch, pad_value=0):\n",
    "    xx = batch\n",
    "    x_lens = [len(x) for x in xx]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    \n",
    "    return xx_pad, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_data = np.array(verify_data,dtype=\"object\")\n",
    "verify_set = VerifyVariableLenDataset(verify_data)\n",
    "verify_loader = DataLoader(verify_set, batch_size=1, shuffle=False, collate_fn=verify_pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data classification\n",
    "import csv\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    num_classes_preds = np.array([0,0,0,0,0])\n",
    "    for x, x_len in verify_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device) \n",
    "        preds, _ = model(x, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        preds = (torch.round(preds).int()).long()\n",
    "        preds = torch.argmax(preds).item()\n",
    "        num_classes_preds[preds]+=1\n",
    "        predictions.append(preds)\n",
    "    print(\"Classes predictions: \",num_classes_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68bf05-d0ee-4ece-894c-17d66e01ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save verified data to csv format\n",
    "with open(\"predictions.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
